import os
from dotenv import load_dotenv
import openai
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

# Charger les variables d'environnement
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise ValueError("La cl√© API OpenAI est manquante. Ajoutez-la dans le fichier .env")

# Initialisation du client OpenAI
openai_client = openai.Client(api_key=OPENAI_API_KEY)

# D√©finition du chemin de la base ChromaDB
CHROMA_DB_DIR = "../vector_database/chroma_db"


# Initialisation de ChromaDB
def get_chroma_collection():
    embeddings = OpenAIEmbeddings()
    return Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings, collection_name="history_france")


# Initialisation de la base vectorielle
vectorstore = get_chroma_collection()


def generate_rag_response(query, top_k=3):
    print("\nüîç Recherche des documents pertinents...")
    results = vectorstore.similarity_search(query, k=top_k)

    if not results:
        return "Je n'ai pas trouv√© d'informations pertinentes pour r√©pondre √† ta question."

    # Construire le contexte avec les chunks r√©cup√©r√©s et inclure les sources
    context = "\n\n".join([
                              f"Source: {doc.metadata.get('title', 'Inconnue')} - {doc.metadata.get('url', 'URL non disponible')}\n{doc.page_content}"
                              for doc in results])

    # Construire le prompt pour GPT-4
    prompt = f"""
    Tu es un expert en histoire. R√©ponds √† la question suivante en utilisant le contexte fourni et tes connaissances.
    Si une information provient d'une source, indique son titre et son URL.

    ### CONTEXTE :
    {context}

    ### QUESTION :
    {query}

    Indique clairement les sources utilis√©es dans ta r√©ponse.
    """

    print("\nüìù Envoi de la requ√™te √† GPT-4...")
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Tu es un assistant sp√©cialis√© en histoire."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=500
    )

    return response.choices[0].message.content
